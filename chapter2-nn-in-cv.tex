\chapter{\label{chap:nn-in-cv}Neural Networks in Computer Vision}

Machine learning (ML) is playing a great role in solving many tasks proposed by the field of computer vision (CV) \cite{ComputerVision}. Analyzing objects from images is a task widely considered as not definable by a set of conditions and therefore, some sort of artificial intelligence (AI) has to be used, as is further discussed in Section~\ref{sec:ml}. When problems got more complicated, ML models had to be increased in depth, which led to the introduction of the term deep learning (DL). 

Neural networks (NNs) became a very powerful computing architecture in machine learning. Section~\ref{sec:cnn} informs how the introduction of convolutional layers to neural networks further increased their efficiency in image processing. Various other operations were also implemented into convolutional neural networks (CNNs) to fit the needs of specific CV challenges.

Deep learning has untangled many problems which were not solvable with other methods. Recognition, being one of these problems, requires a large amount of training data when the model is using supervised learning. This matter is further described in Section~\ref{sec:supervised}. The quality of annotated data available for the training process has a great influence on the model performance and as such became a challenge in the further development of recognition models.

Therefore, learning algorithms that do not require large datasets are being developed. One of these approaches is self-supervised learning introduced in Section~\ref{sec:self-supervised}. It extensively uses a large amount of data without any labels as these are easier to obtain. A model trained in~this manner needs a smaller amount of annotated data to generalize. Such an attribute is~very valuable since labeling can often be done only by hand and is very time-consuming.

\section{\label{sec:ml}Machine Learning}

Computers are very good at solving problems that can be specified with a set of conditions and states, such as a game of chess. The computational power that it possesses allows it to analyze the game many steps ahead. Whereas the human mind finds these tasks very complicated. It is, therefore, no surprise that computers managed to defeat the best chess players in the world. That is why one of the ways to create an artificial intelligence (AI) is focused on the knowledge base approach. The main hypothesis was that every problem can be described in a formal language. After designing a set of logical inference rules, the problem would be solved with just a simple inference.

Unfortunately, when it comes to solving real-world problems, this approach cannot be applied. Humans have immense knowledge about the world and applying it is very subjective and intuitive. It cannot be formalized in any way. The knowledge base AI often did not understand the problem correctly and provided misleading results. Another disadvantage is that the formalization itself was an unwieldy process requiring a large amount of human staff \cite{Goodfellow-et-al-2016}.

Different approaches had to be chosen to solve real-world tasks. Instead of modeling the real world with conditions and rules, probabilistic models with a set of parameters were chosen. Most parameters are to be set automatically based on the data provided to learn the problem's nature. Logistic regression is one of these basic models that provide subjective reasoning based on the information that it learned from previous real-world examples. It~finds a correlation between inputs and various outcomes. The computation involves a weighted sum of the inputs and a non-linear transformation of this sum, illustrated in Figure~\ref{fig:neuron}. Parameters that are adjusted are the weights of each input and a bias, a single number that is added to the sum, which can be also seen as a weight to a constant input of $+1$.

\begin{figure}[!ht]
    \centering
    \begin{tikzpicture}[
            node distance = 8mm and 16mm,
            start chain = going below,
            arrow/.style = {-Latex},
            bloque/.style = {text width=4ex,
                inner sep=1pt,
                align=right,
                on chain
            },
        ]
        
        % inputs
        \node[bloque] (in-0) {\mbox{$+1$}};
        \foreach \i [count=\j] in {1, 2, 3}
            \node[bloque] (in-\j) {$x_{\i}$};
        % output
        \node (out) [circle, draw=black, minimum size=10mm, right=of $(in-1)!0.5!(in-2)$] {\Huge $\Sigma$};
        
        % connections
        \draw[arrow] (in-0) -- (out) node[above, midway] {$b$};
        \foreach \i in {1,...,3}
            \draw[arrow] (in-\i) -- (out) node[above, midway, yshift=0.3mm] {$w_{\i}$};
        
        % activation function
        \node (activation) [rectangle, draw=black, minimum size=6mm, right of=out, xshift=1cm] {\LARGE $\sigma$};
        \draw[arrow] (out) -- (activation);
        
        % output
        \coordinate[right of=activation, xshift=1cm] (output);
        \draw[arrow] (activation) -- (output) node[right] {$h$};
    \end{tikzpicture}
    \caption{Logistic regression as an elementary model in machine learning. $x_{k}, k \in \{1, 2, 3\}$ are the inputs and $h$ is the resulting output that equals the determined probability of the input.}
    \label{fig:neuron}
\end{figure}

Inputs of these models are called features of the data and the performance of the model heavily depends on their representation. If the features correlate with the different outcomes, the model is expected to provide good results. If we wanted to recognize a sports pose and the data provided would be positions of all joints and both eyes in a human body, the task would be fairly easy. After normalizing the scene to always have the same scale and point straight to the eyes, the task gets even simpler. Inputs of such model would be coordinates of mentioned objects and outputs probabilities of each sports pose. This solution introduces another lot more complicated challenge -- the coordinates are hard to obtain without any special tools. What we would like to have is a model that can work on simple image data since obtaining those is affordable.

Images can be described with pixel values and provided to the input but individual pixels have no direct correlation to sports poses, therefore, the predictions would be useless. There is a number of examples of why this is true but perhaps the easiest one is translational dependence. Having the sports pose moved just a few pixels in any direction from where it is expected to be, makes the results incorrect. Additional problems would be caused by shadows, different clothes that the person is wearing, etc.

This obstacle can be overcome by having the ML model discovering not only the mapping of features to outputs but also finding the useful features in the raw data on its own. This makes the model not only work on raw data but also generalizes it to different tasks. For example, not only recognizing sports poses but also vehicles with the same model only trained on different data. Logistic regression is not capable of doing such predictions, some more complex solution has to be found. In computer science, the concept of building complex structures from simple modules is well known and can be used in machine learning as well. By combining many logistic regressions into a structure, a neural network (NN) is created~\cite{Goodfellow-et-al-2016}.

These networks consist of neurons -- logistic regressions. Its non-linear function can be adjusted to fit the needs of a specific task and it is often referred to as an activation function. The capability to solve complex problems arises from the structuring of simple neurons into groups called layers. The key is not doing the mapping of abstract features in one task but dividing it into multiple simple mappings. The input layer, also called the visible layer because its data can be easily observed, provides data to the following layer. The first simple mapping is done by the second layer and each following layer uses the mappings of its predecessor to obtain more complex information from the data. These following layers are also called hidden because their values are not given in the data, they have to be determined by the model. Finally, the last layer provides outputs in a format specified by its activation function.

With tasks becoming more complicated, the number of layers is growing bigger. As there was an increase in the depth of the graph, such networks were called deep neural networks, and their usage was referred to as deep learning (DL).

Neural network for classification can be seen as function $\pmb y=f(\pmb x | \pmb \theta)$ that maps input vector $\pmb x$ to a category $\pmb y$ with parameters of the network given by $\pmb \theta$. It is also possible to decompose the neural network function $f$ to multiple functions, each one representing one layer, applied in the correct order. NN with two hidden layers $f^{(1)}$, $f^{(2)}$ and one output layer $f^{(3)}$ is representing function:

\begin{equation}
    \label{eq:forward-prop}
    \pmb y = f(\pmb x) = f^{(3)}(f^{(2)}(f^{(1)}(\pmb x))).
\end{equation}

Network parameters $\pmb \theta$ are basically weights and biases of each neuron and they are unknown when the model is constructed. The ideal values of parameters cannot be computed in a simple way because of the non-linearity of neural networks that causes most of the loss functions to become non-convex. Loss functions are going to be further explained later in this section. NN parameters have to be somehow initialized and iteratively improved to provide better results. This iterative process is called learning or training and its goal is to approximate some function $f^*$ that provides accurate results for a given problem. That is achieved by finding parameters $\pmb \theta$ that result in such approximation \cite{Goodfellow-et-al-2016}.

Non-linear results of neural networks are achieved with activation functions. There are lots of various functions with different use cases, but two of them are very common. One is a sigmoid function with Equation~\ref{eq:sigmoid}, which maps any real number to a number between 0 and 1. It is often used to represent probability. The other activation function is rectified linear unit (ReLU) (Equation~\ref{eq:relu}) that is linear for any positive number and 0 otherwise. ReLU is very often used in later discussed convolutional neural networks.

\begin{equation}
    \label{eq:sigmoid}
    \sigma (x) = \frac{1}{1 + e^{-x}}
\end{equation}

\begin{equation}
    \label{eq:relu}
    f(x) = \max(0, x)
\end{equation}

The learning algorithm consists of multiple stages that repeat until the model is producing satisfactory results. These stages are explained in detail in the following paragraphs.

\begin{enumerate}
    \item Forward propagating data -- inference.
    \item Computing gradients with back-propagation algorithm.
    \item Calculating learning rate.
    \item Performing learning step of the model with the optimization algorithm.
\end{enumerate}

Evaluation $\pmb y$ of samples $\pmb x$ is computed by forward propagating the samples through the network. That means evaluating all layers in the correct order as illustrated in Equation~\ref{eq:forward-prop}. Correct values $\pmb y^*=f^*(\pmb x)$ are known because the training data are annotated with them. The loss (or cost) function can be used to compute how good the approximation $f$ of $f^*$ is. Loss functions are designed to fit specific tasks and data distribution. When it is necessary to compute some sort of distance on data that probably come from Gaussian distribution, the Mean Squared Error function is often used.

For classification problems, the typical choice is a measure called cross-entropy. It~is based on the Kullback--Leibler divergence, which measures the difference between two probability distributions. Cross-entropy computes the expected number of bits needed to represent data coming from the distribution $p$ while using the distribution $q$ and it is calculated as follows \cite{pml1Book}:

\begin{equation}
    \label{eq:cross-entropy}
    \mathbb{H}(p, q) \triangleq -\sum\limits_{y} p(y) \log q(y).
\end{equation}

Gradients can be computed in many different ways but the most common one for models that are working on large datasets is stochastic gradient descent (SGD). Therefore, this is the only one discussed in this thesis. Generally, a gradient is a vector pointing in the direction of the steepest ascent. By following such a vector, the local maximum can be reached. In machine learning, the thought is often reversed -- the goal is to reach the local minimum, but the main idea remains the same. For the number of samples $m$ and loss function $L$, gradient $\pmb g$ is computed with this equation: 

\begin{equation}
    \label{eq:sgd}
    \pmb g = \frac{1}{m} \sum\limits_{i=1}^{m} \nabla_{\pmb \theta} L(f(\pmb {x}^{(i)}| \pmb \theta), {y}^{*(i)}).
\end{equation}

The direction of the next step is computed but another variable called the learning rate is~still unknown. It represents the size of the step and it has a vast impact on the training performance. One possible solution is to keep the learning rate fixed for the whole training but better results can be achieved with more advanced algorithms. The first improvement can be achieved by computing a specific learning rate for each parameter of the network. The second way to achieve better results is by changing the learning rate throughout the training process.

The update of parameters is done with an optimization algorithm. It uses previously computed gradient $\pmb g$ and other algorithm-specific parameters to update the network's parameters. It usually incorporates the calculation of the learning rate. Very common is the use of the Adam optimization algorithm which also uses the previously mentioned improvements for a more useful learning rate. The algorithm uses the mean and uncentered variance of parameters to adapt the learning rates. The computation goes as follows \cite{kingma2017adam}.

\begin{equation}
    \pmb s = \rho_1 \pmb s + (1 - \rho_1) \pmb g
\end{equation}

\begin{equation}
    \pmb r = \rho_2 \pmb r + (1 - \rho_2) \pmb g \odot \pmb g
\end{equation}

\begin{equation}
    \hat{\pmb s} = \frac{\pmb s}{1 - \rho_1}
\end{equation}

\begin{equation}
    \hat{\pmb r} = \frac{\pmb r}{1 - \rho_2}
\end{equation}

\begin{equation}
    \Delta \pmb \theta = - \epsilon \frac{\hat{\pmb s}}{\sqrt{\hat{\pmb r}} + \delta}
\end{equation}

\begin{equation}
    \pmb \theta = \pmb \theta + \Delta \pmb \theta
\end{equation}

\noindent Where:

$\rho_1, \rho_2$ are exponential decay rates for moment estimates (mean and variance, usually initialized to 0.9 and 0.999 respectively),

$\odot$ is an element-wise product,

$\pmb s$ is an updated biased first-moment estimate,

$\pmb r$ is an updated biased second-moment estimate,

$\hat{\pmb s}$ is a correct bias in the first moment,

$\hat{\pmb r}$ is a correct bias in the second moment,

$\epsilon$ is a step size (usually initialized to 0.001) and

$\delta$ is a small constant used for numerical stabilization (usually initialized to $10^{-8}$).

\vspace{0.2cm}

Another important part of the training process is how the data samples are handled. It~is~possible to update the network's parameters after each sample but also with the whole dataset. The ideal solution is to divide the dataset into minibatches of size ranging from lower tens to higher hundreds of samples. Parameters are then updated with each minibatch. After all minibatches of the dataset have been used for training, the process can start again on the previously minibatches. It is also important to shuffle the data in the dataset and in the minibatches. If the same order of samples was used, the network might have problems with not generalizing enough.

Every time the whole dataset has been handled, one epoch has passed. Training can consist of many epochs, depending on the problem difficulty, network size, and dataset. It~is~important to measure the network's performance on data it has never seen during the training. Once the model's accuracy is not improving and/or loss is approaching nearly zero values, the training will no longer provide better results. Therefore, the dataset should be divided into training and validation data.

\section{\label{sec:cnn}Convolutional Neural Networks}

Convolutional neural networks are a special kind of NNs including at least one layer that is computing convolution. These networks are used for processing data with grid-like topologies, such as sequences and images. This thesis focuses on image data and therefore, only those will be discussed further on, even though the computation can be generalized to other input types.

At first, convolution is discussed as an operation on image data with its important properties. Its usage as a layer in a neural network is explained in detail. Then, other operations important for CNNs are introduced and explained. Once most of the important principles of convolutional neural networks have been mentioned, a specific convolutional neural network architecture is presented.

\subsection{\label{sec:conv-on-image}Convolution on Image Data}

Convolution is a mathematical operation of two functions that produces a third function that describes how one modifies the other in shape. This is a very general definition that is not necessary for image processing and can be made more specific. It is only necessary to consider discrete values of inputs, continuous functions are not used in CNNs. Images usually consist of multiple channels (typically red, green, and blue), but for convolutional neural networks, channels are handled separately. For that reason, images will be discussed as 2-dimensional arrays of numbers only.

Convolution computes a weighted sum of values across a fixed-size area of the image. It takes a 2-D image input and a 2-D array of weights called a kernel. Images can be extended on the edges with padding, which are basically pixels with a value of zero. Since convolution changes the size of the input image, padding is often used to equalize the sizes \cite{dumoulin2016guide}. The resulting 2-D array is often referred to as a feature map and it is computed by multiplying the input value with the corresponding kernel value for all of the overlapping elements and then summed together. After that, the kernel moves one step further on the input and the next value of the feature map is calculated the same way until the whole input is processed. Figure~\ref{fig:convolution} illustrates one step of the computation.

\begin{figure*}[ht]\centering
    \centering
    \begin{tikzpicture}[scale=1]
        \fill[fill={rgb:blue,1;white,1}] (1, 0) -- (3, 0) -- (2, 1) -- (0, 1) -- cycle;
        \draw[fill={rgb:red,1;white,1}] (0, 0.5) -- (1.5, 0.5) -- (0.75, 1.25) -- (-0.75, 1.25) -- cycle;
        \fill[fill={rgb:red,1;blue,2;white,2}] (0.5, 0.5) -- (1.5, 0.5) -- (1, 1) -- (0, 1) -- cycle;
        \draw (0.75, -0.25) -- (3.75, -0.25) -- (2.25, 1.25) -- (-0.75, 1.25) -- cycle;
        \draw (0.5, 0) -- (3.5, 0);
        \draw (0.25, 0.25) -- (3.25, 0.25);
        \draw (0, 0.5) -- (3, 0.5);
        \draw (-0.25, 0.75) -- (2.75, 0.75);
        \draw (-0.5, 1) -- (2.5, 1);
        \draw (1.25, -0.25) -- (-0.25, 1.25);
        \draw (1.75, -0.25) -- (0.25, 1.25);
        \draw (2.25, -0.25) -- (0.75, 1.25);
        \draw (2.75, -0.25) -- (1.25, 1.25);
        \draw (3.25, -0.25) -- (1.75, 1.25);
        
        \draw (0, 0.5) -- (0.25, 2.75);
        \draw (1.5, 0.5) -- (0.75, 2.75);
        \draw (0.75, 1.25) -- (0.5, 3);
        \draw (-0.75, 1.25) -- (0, 3);
        
        \draw[fill={rgb:red,1;blue,1;white,1}] (1, 2) -- (3, 2) -- (2, 3) -- (0, 3) -- cycle;
        \draw[fill={rgb:red,1;blue,2;white,1}] (0.25, 2.75) -- (0.75, 2.75) -- (0.5, 3) -- (0, 3) -- cycle;
        
        \node [xshift=8,yshift=92] {$\Sigma$};
        \node [right=-0.6, yshift=32] {\scriptsize$\times$};
        \node [right=-0.1, yshift=32] {\scriptsize$\times$};
        \node [right=0.4, yshift=32] {\scriptsize$\times$};
        \node [right=-0.4, yshift=25] {\scriptsize$\times$};
        \node [right=0.1, yshift=25] {\scriptsize$\times$};
        \node [right=0.6, yshift=25] {\scriptsize$\times$};
        \node [right=-0.1, yshift=18] {\scriptsize$\times$};
        \node [right=0.4, yshift=18] {\scriptsize$\times$};
        \node [right=0.9, yshift=18] {\scriptsize$\times$};
        
        \draw (0.25, 2.75) -- (2.25, 2.75);
        \draw (0.5, 2.5) -- (2.5, 2.5);
        \draw (0.75, 2.25) -- (2.75, 2.25);
        \draw (0.5, 3) -- (1.5, 2);
        \draw (1, 3) -- (2, 2);
        \draw (1.5, 3) -- (2.5, 2);
    \end{tikzpicture}
    \caption{Example of 2-dimensional convolution with input size 4 (blue), kernel size~3~(red) and padding size 1 (white). The feature map (dark purple) has the same size as the input because of the padding.}
    \label{fig:convolution}
\end{figure*}

The operation of convolution is often denoted with an asterisk $*$ and for input $I$ of size $m \times n$ and kernel $K$, feature map $FM$ is calculated as:

\begin{equation}
    FM(i, j) = (I * K)(i, j) = \sum\limits_m \sum\limits_n I(m, n) K(i-m, j-n).
\end{equation}

Layers that perform convolution are not that different from normal dense layers mentioned in the previous section. The input image is the layer's input, weights are the kernel values and the output of the layer is the feature map. When training is performed, the goal is to find kernel values that produce the best results. Kernels are usually called filters in the CNN context, therefore, this terminology will be used from now on. Each convolutional layer often includes more filters and produces an equal number of feature maps, one feature map from each filter applied to the input. That means 2-D input data are transformed into 3-D data, as there are multiple 2-D feature maps of the same size. The following convolutional layer applies filters to each input and sums the results over each filter. Color images on the input are handled the same way as if each channel was a feature map from a previous convolutional layer, there is no difference between them.

Convolution is a very important operation for image processing because it holds many essential properties. Since it is computed over multiple neighboring values, the context of pixel values is taken into account, not just the single values. That enables pattern recognition in images. Simple patterns such as shadow information and edges are layer by layer combined into more complex patterns until an object detection can be done. Another important property is equivariance, which means that the position of objects in the image plays no role in detection. Finally, convolution has low memory requirements that are not dependent on the input size, only values that are stored are 2-D filter arrays \cite{Goodfellow-et-al-2016}.

\subsection{\label{sec:other-cnn-ops}Additional Important Parts of Convolutional Neural Networks}

Convolutional layers are usually followed by \textit{pooling} layers in CNN architecture. Pooling is~a~function that for each value of the input grid computes a summary statistic of its nearby values. The most common statistics are maximum and average. For example, the max pooling layer with $2 \times 2$ pool size takes a maximum of every $2 \times 2$ region in the image and creates a new image constructed out of the maximum values. A simplified version of this operation is in Figure~\ref{fig:pooling}. In this case, the output size will be smaller than the input size. To keep the size uniform, padding must be added the same way it was added during convolution.

\begin{figure}[!ht]
    \renewcommand{\arraystretch}{1.6}
    \shorthandoff{-}    % To make "-" in hhline work.
    \centering
    \begin{tabular}{|c|c|c|c| c c c}
        \hhline{----~~~}
            \cellcolor{red!30}12 & \cellcolor{red!30}20 & \cellcolor{yellow!30}30 & \cellcolor{yellow!30}0 & & & \\ 
        \hhline{----~--}
            \cellcolor{red!30}8 & \cellcolor{red!30}12 & \cellcolor{yellow!30}2 & \cellcolor{yellow!30}0 & \multirow{2}{*}{$\xrightarrow{2 \times 2 \text{ Max Pool}}$} & \multicolumn{1}{|c|}{\cellcolor{red!30}20} & \multicolumn{1}{c|}{\cellcolor{yellow!30}30} \\
        \hhline{----~--}
            \cellcolor{blue!30}34 & \cellcolor{blue!30}70 & \cellcolor{green!30}37 & \cellcolor{green!30}4 & & \multicolumn{1}{|c|}{\cellcolor{blue!30}75} & \multicolumn{1}{c|}{\cellcolor{green!30}37} \\
        \hhline{----~--}
            \cellcolor{blue!30}75 & \cellcolor{blue!30}13 & \cellcolor{green!30}25 & \cellcolor{green!30}12 & & & \\
        \hhline{----~~~}
    \end{tabular}
    \caption{Max pooling of $4 \times 4$ grid into a $2 \times 2$ grid with pool size also $2 \times 2$. From every region, the maximal value is taken to the output.}
    \label{fig:pooling}
\end{figure}

Pooling layers make the model invariant to small translations. Even when the input has moved a few pixels in some direction, pooled outputs should not change much. Pooling can be also easily used for downsampling of images when the stride is set to 2 or more. Another use case of pooling is handling images of varying sizes because classifiers are accepting only images with fixed size \cite{Goodfellow-et-al-2016}.

\textit{Residual blocks} made a vast impact on the development of convolutional neural networks. While the recognition problems got more complicated and datasets enormous, the need to make CNNs deeper arose. Unfortunately, the performance of the networks did not improve by just adding more layers. Gradients could not be back-propagated all the way to initial layers.

The solution to this problem came with residual blocks that introduced a simple connection that bypassed blocks with convolution and pooling, as shown in Figure~\ref{fig:residual}. This connection symbolizes a simple identity function, it takes the input and outputs it unchanged. When such an identity connection bypasses every convolutional block, the neural network can basically work as an identity function. The same principle can be applied when gradients are computed, therefore, larger gradients are back-propagated to the initial layers \cite{he2015deep-resnet}.

\begin{figure*}[ht!]
    \centering
    
    \tikzstyle{op} = [ellipse, text centered, draw=black]
    \tikzstyle{data} = [rectangle, text centered, draw=black]
    \tikzstyle{arrow} = [thick,->,>=stealth]

    \begin{tikzpicture}[node distance=3cm, scale=0.8, every node/.style={scale=0.8}]    
        \node (input) [data] {$x$};
        \node (layer1) [op, right of=input, xshift=-1cm] {Conv $c$};
        \node (f1x) [data, right of=layer1, xshift=-1cm] {$c(x)$};    
        \node (layer2) [op, right of=f1x, xshift=-1cm] {Pool $p$};
        \node (f2f1x) [data, right of=layer2, xshift=-1cm] {$p(c(x))$};      
        \node (residual) [op, right of=f2f1x, xshift=-1cm] {+};
        \node (output) [data, right of=residual, xshift=-1cm] {$p(c(x)) + x$};

        \draw [arrow] (input) -- (layer1);
        \draw [arrow] (layer1) -- (f1x);
        \draw [arrow] (f1x) -- (layer2);
        \draw [arrow] (layer2) -- (f2f1x);
        \draw [arrow] (f2f1x) -- (residual);
        \draw [arrow] (input.north) to [out=30,in=150] (residual.north);
        \draw [arrow] (residual) -- (output);
    \end{tikzpicture}
    
    \caption{Residual block with bypassed convolutional and pooling layers. Rectangular nodes symbolize data and ellipsoidal nodes are operations.}
    \label{fig:residual}
\end{figure*}

\textit{Dropout} layer is used to deactivate some neurons during the training process and by that, it is trying to simulate different models. For each mini-batch of data, some percentage of neurons have their activation function set to zero. The training step is done as usual: inference, back-propagation, and weight update. Then, for the next mini-batch, different neurons are chosen to produce output with a value of zero. Dropout is a computationally inexpensive way to regularize models \cite{Goodfellow-et-al-2016}.

\section{\label{sec:supervised}Recognition as a Supervised Learning Task}

Recognition is one of many computer vision tasks and it can be further divided into multiple more specific categories. The most common one is classification -- the recognized image is supposed to be assigned a class from a previously known set. This thesis focuses specifically on sports poses classification, however, the other recognition varieties are worth mentioning as well. Detection and segmentation challenges are trying to localize objects in images with a bounding box or pixel-wise, respectively. A very specialized task is a pose estimation where the model is trying to assign a specific structure of connected joints to a human body.

Supervised learning is one of the most general training methods. The main prerequisite is generally a dataset with annotated samples. For the classification task, each data point has to have a class assignment. During the training procedure, the model is trying to assign the correct class to each sample in a mini-batch and compare it to ground truth -- the real class of the sample saved in the dataset. If the model is not successful, the information is back-propagated through the network to improve on the next mini-batch inference.

Nowadays, models are trained on datasets of millions or even billions of data points. Networks with a number of layers well over 100 have enough parameters to be able to classify very complicated images into a large number of categories (even tens of thousands of hierarchically divided ones). An important property of each classifier is its ability to generalize~--~to~classify correctly images it has never seen. Generalization is achieved with training on immense datasets with a large variety of images.

\subsection{ResNet Architecture of Convolutional Neural Networks}

This thesis implements residual network ResNet-50 from \cite{he2015deep-resnet} as a backbone, a structure of~convolutions, pooling, and other operations to obtain information from images. This network was chosen because of its well-known architecture which still provides good results. It can be easily compared to other results since it is widely used across the whole computer vision field. Its depth and number of parameters are not as high as the newer architectures have and therefore, might be easier to train on data. As an alternative, a MobileNet-V2 architecture was also tested \cite{mobilenetv2}. Although it learned faster, its results did not achieve the ResNet's accuracy. It is easily possible to use other CNN architecture as a backbone, the only difference might be the input size that has to be adjusted.

The network uses a well-known structure of convolutional layers followed by pooling layers, where the number of convolutional filters corresponds to the size of the stride and the number of filters in the previous layer. When the stride size is 1, the input and outputs of convolution have also the same size and therefore, the number of filters stays the same. If the image size is decreased with stride size equal to 2, the number of filters doubles. In the beginning, convolution with filter size $7 \times 7$ is used but afterward, standard $3 \times 3$ convolutions are implemented.

The head of the network has a densely connected layer with a softmax activation function. This network introduces residual blocks further explained together with the other mentioned concepts in Section~\ref{sec:cnn}. This allows the network to have higher tens or even lower hundreds of layers and still be able to learn well. The network also uses batch normalization after each convolution but before the activation. With the exception of the network's output, Rectified Linear Unit is used as an activation function. An example of the architecture in a simplified form is in Table~\ref{tab:resnet}.

\begin{table}[!ht]
    \centering
    \footnotesize
    \begin{tabular}{ |r|m{5em}|m{9em}|l| }
        \hline
            Repeated & Layer & Settings \\ 
        \hline
        \hline
            $1 \times$ & Input & \mbox{Size: (224, 224,3)}\\
        \hline
            $1 \times$ & Conv2D & \mbox{Filter count: 64} \mbox{Kernel size: (7, 7)} \mbox{Stride size: (2, 2)}\\
        \hline
            $6 \times$ & Conv2D & \mbox{Filter count: 64} \mbox{Kernel size: (3, 3)} \mbox{Stride size: (1, 1)}\\
        \hline
            $1 \times$ & Conv2D & \mbox{Filter count: 128} \mbox{Kernel size: (3, 3)} \mbox{Stride size: (2, 2)}\\
        \hline
            $7 \times$ & Conv2D & \mbox{Filter count: 128} \mbox{Kernel size: (3, 3)} \mbox{Stride size: (1, 1)}\\
        \hline
            $1 \times$ & Conv2D & \mbox{Filter count: 256} \mbox{Kernel size: (3, 3)} \mbox{Stride size: (2, 2)}\\
        \hline
            $11 \times$ & Conv2D & \mbox{Filter count: 256} \mbox{Kernel size: (3, 3)} \mbox{Stride size: (1, 1)}\\
        \hline
            $1 \times$ & Conv2D & \mbox{Filter count: 512} \mbox{Kernel size: (3, 3)} \mbox{Stride size: (2, 2)}\\
        \hline
            $5 \times$ & Conv2D & \mbox{Filter count: 512} \mbox{Kernel size: (3, 3)} \mbox{Stride size: (1, 1)}\\
        \hline
            $1 \times$ & Dense & \mbox{Units: 1000} \mbox{Activation: Softmax} \\
        \hline
        \end{tabular}
    \caption{ResNet-34 architecture with only the main layers mentioned together with their settings. Some layers are repeatedly used after each other, the number of repetitions is in the first column. Residual connections are over every 2 convolutional layers except the first one.}
    \label{tab:resnet}
\end{table}

\subsection{Current Advances in Sports Pose Recognition}

Most of the current research focuses on sports pose estimation which is a different task than sports pose classification done in this thesis. Researchers also tried yoga pose recognition from body contours but the variety of poses did not draw near to all possible options \cite{yoga-posture-recognition}. However, with the publishing of the Yoga-82 dataset, results of different supervised-trained models were also analyzed \cite{verma2020yoga}. Summarized in Table~\ref{tab:yoga82-results} are the results of classifying images from the Yoga-82 dataset with various CNN models.

\begin{table}[ht]
    \centering
    \begin{tabular}{l r r r r}
        \hline
        Architecture & Depth & Parameters & Top-1 Accuracy & Top-5 Accuracy \\
        \hline
        ResNet-101 & 101 & 42.72 M & 65.84 & 84.21 \\
        DenseNet-169 & 169 & 12.60 M & 74.73 & \textbf{91.44} \\
        DenseNet-201 & 201 & 18.25 M & \textbf{74.91} & 91.30 \\
        MobileNet-V2 & 88 & 2.33 M & 71.11 & 88.50 \\
        ResNeXt-50 & 50 & 23.15 M & 68.45 & 86.42 \\
        \hline
    \end{tabular}
    \caption{Performance of widely-known CNN architectures on Yoga-82 dataset using third-level classes from \cite{verma2020yoga}.}
    \label{tab:yoga82-results}
\end{table}

The Yoga-82 dataset contains 82 third-level classes of yoga poses grouped into 20 second-level classes that are further merged into 6 first-level classes. The poses are grouped according to the posture and pose look. Of course, not all poses can be easily assigned to one of the 82 third-level classes, some variation has to be taken into account. The hierarchy of classes can be used to improve the classification or to estimate a pose type with higher accuracy -- reported first, second, and third-level accuracy is $89.81\,\%$, $84.59\,\%$, and $79.08\,\%,$ respectively, for top-1 accuracy on DenseNet-201.

Yoga as a sport includes an extensive amount of poses with a variety that stands out amongst other sports. The poses can be also sorted into groups and thus create a hierarchy that can be further used for classification as can be seen in \cite{verma2020yoga}.

\section{\label{sec:self-supervised}Self-Supervised Learning for Computer Vision}

Self-supervised learning is a method of training a model first to learn data representations on~unannotated data and then to use annotated data to train another model for classification of the representations. The first model learns patterns in the data and how to represent the needed information in it with a latent vector. There is no need for any labels since the data itself is used for supervision. Means of obtaining the supervision differ upon the task and data available. The second model needs to use an annotated dataset to assign classes to latent vector output by the first model. Classification is made easier with the data being represented efficiently and there is no need for large annotated datasets to achieve good generalization properties of the model.

There are various ways to use the image data itself as supervision. For instance, it~is~possible to use a small distortion on the original data and expect it to not change its meaning. With this, different images that are bound together are created automatically. Self-supervision can also be used for colorization tasks, the original color images are easily converted into grayscale images and the model's goal is to colorize it to match the original sample. Another common challenge is generating missing image data, which is done with context encoders. Some part of the image is cropped out and the encoder is trying to fill it~in~to its previous form.

This thesis uses models called time-contrastive networks (TCNs) introduced in \cite{sermanet2018timecontrastive}. Self-supervision is achieved by using multiple cameras to film a scene from different viewpoints. After the videos are synchronized, frames with the same timestamp but from different cameras should still produce latent vectors fairly close to each other. When the timestamps are different (and the scene changed), latent vectors should be further from each other even when filmed from an identical viewpoint.

Self-supervised models use different loss functions that suit their specific approach to~solving the problem. As described in the previous paragraphs, self-supervised learning has many different forms and, therefore, architectures vary in many ways. The time-contrastive network has to learn to represent the object in the image independently of the viewpoint. Images from the same viewpoint can differ just a little in time but their image embeddings should be different if the observed object changed. Whereas images from different viewpoints at the same time can be entirely different, only the observed object is constant. Therefore, their embeddings should be reasonably similar. Such a challenge can be solved with a loss function called the triplet loss.

\subsection{\label{sec:triplet-loss}Triplet Loss Function}

The triplet loss function pushes embeddings of similar data closer together and pulls embeddings of diverse data further apart. Its main goal is learning data representation in a $d$-dimensional Euclidean space. Inputs of the function are 3 data embeddings: anchor, positive, and negative. Anchor and positive should be closer to each other than anchor and negative. It can be thought of as anchor and positive belonging to the same class and negative to~a~different one \cite{facenet-triplet-loss}.

Data point $x$ has an embedding $f(x) \in \mathbb{R}^d$ which is additionally constrained to live on~a~unit hypersphere -- ${|| f(x) ||}_2 = 1$. Triplet $i$ consists of anchor image $x^{a}_{i}$, positive image $x^{p}_{i}$ and negative image $x^{n}_{i}$. The goal of the network is for inequality  \ref{eq:triplet-distance} to held true with condition \ref{eq:triplet-condition}.

\begin{equation}
    \label{eq:triplet-distance}
    {|| f(x^{a}_{i}) - f(x^{p}_{i}) ||}^{2}_{2} + \alpha < {|| f(x^{a}_{i}) - f(x^{n}_{i}) ||}^{2}_{2}
\end{equation}

\begin{equation}
    \label{eq:triplet-condition}
    \forall ( f(x^{a}_{i}), f(x^{p}_{i}), f(x^{n}_{i})) \in \mathcal{T}
\end{equation}

\noindent Where $\alpha$ is a margin that is enforced between positive and negative pairs and $\mathcal{T}$ is the set of all possible triplets, $|\mathcal{T}| = N$.

The triplet loss to be minimized is then:

\begin{equation}
    \label{eq:triplet-loss1}
    L = \sum\limits^{N}_{i}
    \left[
    {|| f(x^{a}_{i}) - f(x^{p}_{i}) ||}^{2}_{2} - {|| f(x^{a}_{i}) - f(x^{n}_{i}) ||}^{2}_{2} + \alpha
    \right]_{+}.
\end{equation}

\noindent And more often it is used as:

\begin{equation}
    \label{eq:triplet-loss2}
    L = \sum\limits^{N}_{i}
    \max{(0, {|| f(x^{a}_{i}) - f(x^{p}_{i}) ||}^{2}_{2} - {|| f(x^{a}_{i}) - f(x^{n}_{i}) ||}^{2}_{2} + \alpha)}.
\end{equation}

\newpage The triplet selection is an important part of the whole training process. When the constraint \ref{eq:triplet-condition} is easily met, the triplet has not improved the model at all and therefore, it~will converge slower. The ideal triplets (hard positive and hard negative, respectively) are satisfying these two conditions \cite{facenet-triplet-loss}:

\begin{equation}
    \label{eq:triplet-sel1}
    x^{p}_{hard} = \argmax_{x^{p}_{i}}{{|| f(x^{a}_{i}) - f(x^{p}_{i}) ||}^{2}_{2}},
\end{equation}

\begin{equation}
    \label{eq:triplet-sel2}
    x^{n}_{hard} = \argmin_{x^{n}_{i}}{{|| f(x^{a}_{i}) - f(x^{n}_{i}) ||}^{2}_{2}}.
\end{equation}

In this thesis, with the time-contrastive learning method, hard triplets cannot be computed in any way. Hard positive pair is enforced by making the viewpoints of the images as different as possible. Since anchor and positive images are taken from different viewpoints, the background of the scene and light conditions can vary. Also, just by filming the scene from different angles, the object can look completely different. The hard negative pair condition can be fulfilled by choosing an image from the same viewpoint as for the anchor image but with a slightly different timestamp. How much should the timestamp differ, depends on the scene itself. If it is very dynamic, the images can be just a few frames apart, if it stays the same for a longer time, a different approach has to be chosen. One possibility to choose a hard negative pair from a video is by computing optical flow from the video and using it~to~detect movement. This procedure is thoroughly discussed in Section~\ref{sec:motion-detect}.

\subsection{\label{sec:other-loss}Other Metric Learning Loss Functions}

Triplet loss is not the only possible loss function that can be used to construct an embedding space with a neural network. There are several other functions for metric learning that can be applied to the same task that is being solved in this thesis. These alternative functions are proposed as a possibility for future work on the project and are not implemented.

Lifted Structure Loss is a good candidate for an alternative to the triplet loss function \cite{lifted-structure}. It uses a similar format to the triplet loss's training data: anchor, positive, and negative samples. The difference is that it utilizes multiple negative samples at once and thus provides faster convergence. It is fairly easy to provide a higher number of negative samples since frames before and after the timestamp of anchor-positive pairs from all viewpoints are candidates for negatives.

Multi-Class N-Pair Loss is very similar to lifted structure loss in the sense that it uses multiple negative samples but it differs in what it tries to optimize \cite{multiclass-NIPS2016_6b180037}. It computes cosine similarity between features of the data points and tends to be scale-invariant. 

While triplet and lifted structure losses both use relative distance as a metric, angular loss accounts for the angle at the negative edge of the triplet triangle \cite{angular-loss}. It drags negative data points away from the anchor-positive pair. The pair is on the other hand pushed closer together. This metric also benefits from scale invariance. The advantage over triplet loss is an easier setting of margin as a hyperparameter. The margin for triplet loss depends on the intra-class variance of data while the margin angle for angular loss is invariant of such property.
