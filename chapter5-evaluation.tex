\chapter{\label{chap:evaluation}Evaluation of Models Trained with Self-Supervision}

Models trained with self-supervision can be evaluated at two stages. Obviously, the accuracy of classification of given input is one way to do so. The other way is to evaluate embeddings either with loss function used for training of the encoder or visually after dimensionality reduction. All of these approaches are discussed in this chapter.

At first, the sports pose embeddings are visually analyzed in section \ref{sec:visual}. Then, section \ref{sec:evaluate-encoder} presents encoder's accuracy on validation dataset and how it is effected by the number of dimensions of the embedding space. Finally, accuracy of the classification itself on validation dataset is compared to supervised model with same architecture in section \ref{sec:comparison}. Results of each evaluation are discussed in their respective sections. Experiments were done fairly and no results were cherry-picked.

\section{\label{sec:visual}Visual Analysis of Latent Space}

The latent space has well over 3 dimensions and therefore cannot be easily visualized. Typically, embeddings of more complex information such as sports pose can range from 64 to 512 dimensions. Vectors representing them usually satisfy the constraint of living on a unit hypersphere. Analysing data visually can help understand patterns in them and detect emerging problems. When the dimensionality is decreased to only 2 dimensions, a lot of information can be lost. Therefore, the challenging task for the projecting algorithm is to drop the non-necessary information and preserve the patterns in data.

The elemental method for dimension reduction is Principal Component Analysis (PCA). It computes a new basis of the vector space to maximize the data variance. After projecting the data into the new basis, only 2 or 3 dimensions with the highest variance can be taken into account and the rest is ignored. Finally, such data can be plotted and reviewed. Other possible projection is Linear Discriminant Analysis (LDA) that also takes into account the label of each data point and is trying to find basis that allows for the best linear separation of classes.

Whereas the previously mentioned algorithms allowed for computing the precise results, more complex methods for dimension reduction are based on iterative approaches to find the best approximation of the ideal state since it cannot be computed directly. Widely used algorithm for this task is t-distributed Stochastic Neighbor Embedding (t-SNE). It puts data points into pairs and tries to attract those that are similar and repel the dissimilar ones. Other iterative method is Uniform Manifold Approximation and Projection (UMAP) which also non-linearly projects data into 2D or 3D. I chose to use t-SNE as the dimensionality reduction algorithm because it was able to find patterns in the data embeddings better than the other algorithms.

\subsection{Dimensionality Reduction with t-distributed Stochastic Neighbor Embedding}

t-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction method suited for displaying embedding vectors in two or three-dimensional space \cite{tsne-vandermaaten08a}. It is based on Stochastic Neighbor Embedding (SNE) but it uses also t-distribution instead of only Gaussian distribution \cite{sne-NIPS2002_6150ccc6}. t-distribution has heavier tails in comparison to Gaussian distribution and therefore, it solves one of the problems of SNE, which was centering the data points into one place in the low dimensions and not preserving the gaps between them.

t-SNE algorithm starts with a random initialization of projected data points in the targeted 2 or 3-dimensional space. It places them fairly close to each other to allow for patterns emerging in higher scale. Then two similarity distributions are constructed -- one from points in the source high-dimensional space, the other from points in the destination low-dimensional space. Both distributions are constructed from distances between all pairs of data points in their respective spaces. Then, Kullback-Leibler divergence of joint distribution P in the high-dimensional space and Q in the low-dimensional space is minimized:

\begin{equation}
    \label{eq:tsne-kl}
    C = KL(P||Q) = \sum \limits_{i} \sum \limits_{j} p_{i j} \log \frac{p_{i j}}{q_{i j}}.
\end{equation}

Distances of data points from themselves $p_{i i}$ and $q_{i i}$ are set to zero. $p_{i j}$ and $p_{j i}$ are averaged in order to preserve symmetry $p_{i j} = p_{j i}$. Each data point pair is assigned a probability from Gaussian distribution with mean set to coordinates of point $i$ and variance computed from the density of other points around it. The distance of point $j$ from $i$ is projected to the Gaussian distribution and $p_{i j}$ equals the given probability, calculated as:

\begin{equation}
    \label{eq:tsne-p}
    p_{i j} = \frac{e^{-|| {y}_{i} - {y}_{j} ||^2}}{\sum \limits_{k \neq l} e^{-|| {y}_{k} - {y}_{l} ||^2}}.
\end{equation}

\noindent Probabilities $q_{i j}$ are obtained from Student's t-distribution with one degree of freedom with a similar approach to the $p_{i j}$. The formula is as follows:

\begin{equation}
    \label{eq:tsne-q}
    q_{i j} = \frac{(1 + || {y}_{i} - {y}_{j} ||^2)^{-1}}{\sum \limits_{k \neq l} (1 + || {y}_{i} - {y}_{j} ||^2)^{-1}}.
\end{equation}

\noindent Finally, gradient of the Kullback-Leibler divergence between P and Q is computed with:

\begin{equation}
    \label{eq:tsne-gradient}
    \frac{\partial C}{\partial y_{i}} = 4 \sum \limits_{j} (p_{i j} - q_{i j}) (y_{i} - y_{j}) (1 + || {y}_{i} - {y}_{j} ||^2)^{-1}.
\end{equation}

\subsection{Analysis of embeddings with t-distributed Stochastic Neighbor Embedding}

The encoder presented in section \todo{number} transforms image of sports pose into 256-dimensional vector embedding. The goal is to have this embedding describe only the sports pose and ignore background of the scene and look of the person doing the pose. Sports poses similar to each other should be closer to each other in the embedding space than poses that are completely different. If only one arm moved from one image frame to another, their embeddings should be very similar. The same pose performed by other person in a different place and even photographed from a different angle should have the same or at least very similar embedding.

The whole Directions Dataset introduced in section \ref{sec:dataset-labels} includes 3804 divided into 4 classes according to arm positions of the person -- each arm is either pointing down or up and therefore, the corresponding classes are named \texttt{down-down}, \texttt{down-up}, \texttt{up-down} or \texttt{up-up}. Positions where both arms are in a downward direction should be relatively far from each other while the other 2 classes can be placed somewhere in between the edge cases. The dataset consists of 10 scenes, each filmed from 3 angles. Projected embedding are displayed in the figure \ref{fig:t-sne}.

\begin{figure}[!ht]
    \centering
    \begin{tikzpicture}
        \pgfplotsset{scale=1.5}
        \begin{axis}[
            title={Embeddings of sports poses after t-SNE projection},
            xlabel={x},
            ylabel={y},
            legend pos=outer north east,
            xmajorgrids=true,
            ymajorgrids=true,
            grid style=dashed,
        ]
            \addplot[
                only marks,
                color=yellow,
                mark=+,
                mark options={scale=0.7}
            ]
                table[col sep=comma, header=true] {figures/t-sne/down-down.csv};
            \addplot[
                only marks,
                color=orange,
                mark=+,
                mark options={scale=0.7}
            ]
                table[col sep=comma, header=true] {figures/t-sne/down-up.csv};
            \addplot[
                only marks,
                color=red,
                mark=+,
                mark options={scale=0.7}
            ]
                table[col sep=comma, header=true] {figures/t-sne/up-down.csv};
            \addplot[
                only marks,
                color=purple,
                mark=+,
                mark options={scale=0.7}
            ]
                table[col sep=comma, header=true] {figures/t-sne/up-up.csv};
            \legend{down-down, down-up, up-down, up-up}
        \end{axis}
    \end{tikzpicture}
    \caption{Embeddings of all 3804 samples from the Directions Dataset projected from 256 dimensions to 2D with the t-SNE algorithm. Data points are colored according to their class. Parameters of the t-SNE were 600 iterations, perplexity 16, learning rate 10 and the algorithm ran without any supervision based on sample labels.}
    \label{fig:t-sne}
\end{figure}

The projection clearly shows a number of clusters of different sizes, each consisting of data points from all 4 classes. Each cluster is probably a representative of single viewpoint of a scene with some of them being closer to each other or even almost merged together. This shows that the encoder model is not capable of generalizing over different scenes nor viewpoints. One possible explanation of such behaviour is not using diverse enough dataset.

When focusing on each cluster individually, the data point distribution holds relations for similar sports poses. Classes \texttt{down-down} and \texttt{up-up} are usually far apart from each other within the cluster and while the \texttt{down-up} and \texttt{up-down} are between them. Some images of poses contain arms pointing almost perfectly horizontally and their classification cannot be precise. These cases have to be taken into account.

\section{\label{sec:evaluate-encoder}Evaluation of Encoder on Validation Dataset}

The encoder itself is just a single component in the whole model that performs the classification. Its performance cannot be easily measured like a normal classifier -- by counting how many of the validation data were correctly assigned their class. There are no ground truths to the inputs, no image of a sports pose has a correct nor false embedding. The only way to measure the encoder's performance is by comparing one embedding to another. If the objective is to have similar sports poses close to each other in the embedding space, the distances of embeddings can be compared.

The encoder is trained with a triplet loss function whose aim is to have two embeddings of different images of the same sports pose closer to each other than two embeddings of a distinct pose. The distance between correct and false pairs should also be greater than some fixed value called margin. Therefore, the same function can be also used to evaluate the encoder. The only difference is that the margin is set to 0, whereas during the learning process, the value is above 0.

The objective of this experiment was to evaluate the performance of encoder model on different dimensionalities of the embedding space. Numbers of dimensions used for testing were 16, 32, 64, 128, 256 and 512. Margin of the triplet loss was set to $0.1$. The model was trained for 50 epochs on the same training data and evaluated on the validation subset after each epoch. The dataset used for this experiment is Upper Body Dataset from section \ref{sec:dataset-upper-body}. Dataset was split so that $90 \%$ of it was used for training and $10 \%$ for validation. Each model was trained $5 \times$ on the same dataset but shuffled with a distinct seed. To provide consistency of training data between different embedding space dimensionalities, the seed had the same value from 0 to 4 in the 5 runs. The best validation accuracy of all epochs was taken as the model's accuracy. The obtained results are presented in form of boxplots in the figure \ref{fig:encoder-embedding}. Format of the boxplots is from bottom: minimum, first quartile, median, third quartile and maximum.

\begin{figure}[!ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            title={Comparison of validation accuracy on different embedding dimensionalities},
            ylabel={Accuracy on validation dataset},
            xlabel={Embedding dimensionality},
            boxplot/draw direction=y,
            xmode=log,
            log basis x={2},
            xmin=8, xmax=1024,
            xtick={16,32,64,128,256,512},
            log ticks with fixed point,
            ymin=0.95, ymax=1,
            ytick={0.95, 0.96, 0.97, 0.975, 0.98, 0.99, 1},
            y tick label style={
                /pgf/number format/.cd,
                    precision=3,
                /tikz/.cd
            },
            ymajorgrids=true,
            xmajorgrids=true,
            grid style=dashed,
            cycle list={{red}}
        ]
            \addplot+ [
                boxplot prepared={
                    draw position=16,
                    lower whisker=0.9622,
                    lower quartile=0.96415,
                    median=0.9762,
                    upper quartile=0.9801,
                    upper whisker=0.9801,
                    box extend=4
                }
            ] coordinates {};
            \addplot+ [
                boxplot prepared={
                    draw position=32,
                    lower whisker=0.9603,
                    lower quartile=0.9606,
                    median=0.9688,
                    upper quartile=0.9782,
                    upper whisker=0.9818,
                    box extend=8
                }
            ] coordinates {};
            \addplot+ [
                boxplot prepared={
                    draw position=64,
                    lower whisker=0.9723,
                    lower quartile=0.9746,
                    median=0.9772,
                    upper quartile=0.9806,
                    upper whisker=0.9824,
                    box extend=16
                }
            ] coordinates {};
            \addplot+ [
                boxplot prepared={
                    draw position=128,
                    lower whisker=0.9671,
                    lower quartile=0.9689,
                    median=0.9714,
                    upper quartile=0.97465,
                    upper whisker=0.9753,
                    box extend=32
                }
            ] coordinates {};
            \addplot+ [
                boxplot prepared={
                    draw position=256,
                    lower whisker=0.9593,
                    lower quartile=0.96335,
                    median=0.9688,
                    upper quartile=0.97445,
                    upper whisker=0.9788,
                    box extend=64
                }
            ] coordinates {};
            \addplot+ [
                boxplot prepared={
                    draw position=512,
                    lower whisker=0.9642,
                    lower quartile=0.96435,
                    median=0.9691,
                    upper quartile=0.97445,
                    upper whisker=0.9753,
                    box extend=128
                }
            ] coordinates {};
        \end{axis}
    \end{tikzpicture}
    \caption{When an image of a sports pose is encoded into a vector in an embedding space, its dimensionality can play a role in the performance of the model. The encoder model was tested on a number of dimensions between 16 a 512. The best median accuracy on validation data had an embedding space with 64 dimensions.}
    \label{fig:encoder-embedding}
\end{figure}

The highest median accuracy on validation dataset achieved model that encoded the sports pose images into 64-dimensional vectors. Embedding spaces with 32 and 16 dimensions might achieve comparable accuracy in some runs but their variance is very high. This suggests that the model is not always capable of finding efficient enough encoding to store all the information about the pose, even though it might be possible. Models producing encodings with 64 and more dimensions show less variance in accuracy which advocates for their ability to save all the necessary information in the embedding. Their median accuracy declines with rising dimensionality. That is a corresponding incident since encoding pose into a higher-dimensional space is a more difficult task and with rising complexity the accuracy drops. From these assumptions an embedding space with 64 dimensions provides the best results on Upper Body Dataset.

Dataset with very high diversity in sports poses requires more information to be stored and therefore an embedding space with more dimensions. Correctly chosen size of the embedding space can influence the performance of the model and the time it requires for fitting on dataset. Therefore, it is advised to tune this hyperparameter to match the dataset complexity.

\section{\label{sec:comparison}Comparison of Self-Supervised and Supervised-Trained Models}

The main advantage of models trained in a self-supervised manner is their ability to perform well with dataset containing smaller number of labeled data than what would supervised training needed. This advantage is shown in evaluation done on different-sized datasets in the following experiments.

The model described in the previous chapter (\todo{chapter link}) is used as the self-supervised learning benchmark. For supervised learning representative, the ResNet50 model is chosen and it is not alternated in any way \cite{he2015deep}. The architectures of both networks are very similar as can be seen in the table \ref{tab:self-supervised-vs-supervised}, their backbones are the same, only their last few layers are different, and therefore, the evaluations should not be distorted because of the models.

\begin{table}[!ht]
    \begin{center}
        \begin{tabular}{ |c|c||c|c| }
            \hline
                \multicolumn{2}{|c||}{Self-Supervised Model} & \multicolumn{2}{c|}{Supervised Model} \\
            \hline
            \hline
            Description & Layer -- Shape & Layer -- Shape & Description \\
            \hline
            \hline
                Image & Input -- (224, 224, 3) & Input -- (224, 224, 3) & Image \\
            \hline
                \makecell{Backbone \\ ResNet50} & \makecell{Padding -- (230, 230, 3) \\ \vdots \\ Pooling -- (2048)} & \makecell{Padding -- (230, 230, 3) \\ \vdots \\ Pooling -- (2048)} & \makecell{Backbone \\ ResNet50} \\
            \hline
                & Dense -- (256) & \multirow{4}{*}{Dense -- (4)} & \multirow{4}{*}{Label} \\
            \cline{1-2}
                Embedding & L2 Normalize -- (256) & & \\
            \cline{1-2}
                & Dense -- (64) & & \\
            \cline{1-2}
                Label & Dense -- (4) & & \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Comparison of architectures of self-supervised and supervised models. Their input, backbone and output are identical, only the top of self-supervised model is adjusted for the self-supervised training.}
    \label{tab:self-supervised-vs-supervised}
\end{table}

Since each model is trained in a different way, it is not trivial to set the borderline for number of epochs used for training. For that reason, each model was trained for sufficient number of epochs after which it no longer improved on validation data. The self-supervised model consists of two parts -- the encoder which creates the embedding from an image and the recognizer which classifies the pose from the embedding. The encoder was trained for 30 epochs and the recognizer for 20 epochs. The encoder was trained once and stayed the same for the whole experiment while the recognizer was fitted for every dataset sample. The supervised model was trained for 50 epochs on each dataset sample.

Dataset used for these experiments is thoroughly described in section \ref{sec:dataset-sports-poses} -- Upper Body Dataset. It contains 3804 images of 1268 poses, each captured from 3 different angles. The poses are not necessarily unique but the images differ in background and clothes of the person. Overall, there are 10 different scenes with 2 possible backgrounds and different clothing of the person in each of the scenes. The poses represent possible movements of person's arms in all directions and joints' bendings, other parts of the body such as torso, head or legs are not moving.

First experiment is done on the Directions Dataset which contains 4 classes that differ in position of arms -- left or right arm is pointing either down or up. Since some positions may be questionable, the rule of thumb during labeling of data was whether the wrist is above or below the corresponding shoulder. The second experiment was done on Bent Dataset with 16 classes that extended the Directions Dataset with one more attribute -- whether the arms are bent or not.

While the first experiment only evaluates the accuracy of choosing the correct class (top-1 accuracy), the second one evaluates also accuracy whether the correct class is within 3 of the most probable outcomes (top-3 accuracy). Decision to provide results in this format was made based on the number of classes in used datasets.

Both of tested models were fitted on datasets of different sizes and then their accuracy on never-seen validation data was evaluated. Before each training the whole dataset was shuffled and then divided into training and validation subsets. The portion of data used for training was the changing variable and it ranged from $0.9$ to $0.025$. The rest of the data where always used for validation. This approach decreases variance in the training dataset and concurrently increases it in the validation dataset. Because of this, model's ability to generalize well is displayed.

To keep the experiment fair, dataset for each experiment was shuffled with the same seed for both models and, therefore, they had the same data for training and evaluating. For each dataset split portion, 10 runs of fitting and evaluating were done, each one with different seed. Seeds were chosen deterministically as integers from $0$ to $9$. Accuracy on validation data was then averaged over all 10 runs to get the final accuracy of the model for a given fraction of training data.

The results of the experiment on Directions Dataset are shown in the figure \ref{fig:self-supervised-vs-supervised-directions}. For training dataset portions down to $15 \%$ (which equals $570$ of the $3804$ images used for training), the performances of self-supervised and supervised models are on a par. When the training dataset portion decreases to $12.5 \%$ ($475$ of $3804$ images) the supervised model starts to degrade and with just $2.5 \%$ images it approaches accuracy $25 \%$ which is for 4 classes basically a random guess. While the self-supervised model keeps its accuracy above $60 \%$ even when trained on $2.5 \%$ data which equals to $95$ images for training and $3709$ for validation.

Experiment number 2 with training and evaluation done on Bent Dataset provides not only top-1 but also top-3 accuracy. The results are presented in the figure \ref{fig:self-supervised-vs-supervised-bent}. While the previous experiment included only 4 classes, the Bent Dataset consists of 16 classes and this difference made an impact on the results. The accuracy of both models dropped down by approximately 0.2 overall. The self-supervised model performed better than supervised model in all provided dataset splits. This is a display of one of the advantages of self-supervised learning, it adapts better to a higher number of classes since it already learned the key features on unlabeled data. When the amount of training samples approaches less than 10 for each class overall (portion of 0.025), the accuracy of supervised model drops significantly while the self-supervised model's accuracy stabilizes.

The top-3 accuracy reveals important information about the supervised model when trained on $10 \%$ of the Bent Dataset -- part of the plot with dashed line. The predicted results degrade in a way that the top-3 accuracy equals to $1.0$ for more and more of the experiment runs and the top-3 accuracy suddenly rises. Since the top-1 accuracy declines, the model is not performing better, rather it found some workaround that produces these improbable results. The model is also not trained to maximize the top-3 accuracy, it is trying to minimize the loss function. Self-supervised learning clearly provides results with higher or equal accuracy for all experiments and shows its advantages mainly on datasets with low number of training samples.

\begin{figure*}[!ht]
    \begin{subfigure}[b]{\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                title style={align=center},
                title={Comparison of self-supervised and supervised models' accuracy\\on Directions Dataset with 4 classes},
                xlabel={Portion of dataset used for training},
                ylabel={Accuracy on validation dataset},
                xmin=0, xmax=1,
                ymin=0, ymax=1,
                xtick={0,0.1,0.2,0.4,0.6,0.8,1},
                ytick={0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1},
                legend pos=south east,
                xmajorgrids=true,
                ymajorgrids=true,
                grid style=dashed,
            ]
                \addplot[color=red,mark=o]
                    coordinates {(0.025,0.6164)(0.050,0.6879)(0.075,0.6524)(0.100,0.6716)(0.125,0.7293)(0.150,0.7138)(0.175,0.6664)(0.200,0.7463)(0.300,0.8496)(0.400,0.8769)(0.500,0.9125)(0.600,0.9198)(0.700,0.9204)(0.800,0.9286)(0.900,0.9326)};
                \addplot[color=blue,mark=square]
                    coordinates {(0.025,0.2806)(0.050,0.2944)(0.075,0.3713)(0.100,0.3554)(0.125,0.4986)(0.150,0.6705)(0.175,0.7746)(0.200,0.7376)(0.300,0.8618)(0.400,0.8828)(0.500,0.8976)(0.600,0.9113)(0.700,0.9140)(0.800,0.9175)(0.900,0.9174)};
                \legend{Self-Supervised, Supervised}
            \end{axis}
        \end{tikzpicture}
        \caption{Self-supervised model displays better accuracy for small training datasets. With the dataset size getting larger, both models perform similarly.}
        \label{fig:self-supervised-vs-supervised-directions}
        \vspace{0.5cm}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \begin{tikzpicture}
        	\begin{axis}[
                    title style={align=center},
        		title={Comparison of self-supervised and supervised models' accuracy\\on Bent Dataset with 16 classes},
        		xlabel={Portion of dataset used for training},
        		ylabel={Accuracy on validation dataset},
        		xmin=0, xmax=1,
        		ymin=0, ymax=1,
        		xtick={0,0.1,0.2,0.4,0.6,0.8,1},
        		ytick={0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1},
        		xmajorgrids=true,
        		ymajorgrids=true,
        		grid style=dashed,
        		legend pos=south east,
        		legend style={nodes={scale=0.8}},
        	]
        		\addplot[color=red, mark=o]
        			coordinates {(0.025,0.4029)(0.050,0.4260)(0.075,0.4049)(0.100,0.4819)(0.125,0.4855)(0.150,0.5456)(0.175,0.5651)(0.200,0.5888)(0.300,0.6678)(0.400,0.7238)(0.500,0.7365)(0.600,0.7602)(0.700,0.7710)(0.800,0.7799)(0.900,0.7947)};
        		\addplot[color=red, mark=*]
        			coordinates {(0.025,0.5398)(0.050,0.5800)(0.075,0.5846)(0.100,0.6458)(0.125,0.6678)(0.150,0.7313)(0.175,0.7618)(0.200,0.7701)(0.300,0.8490)(0.400,0.8929)(0.500,0.8986)(0.600,0.9189)(0.700,0.9307)(0.800,0.9353)(0.900,0.9416)};
        		\addplot[color=blue, mark=square]
        			coordinates {(0.025,0.1414)(0.050,0.2090)(0.075,0.2441)(0.100,0.2404)(0.125,0.2928)(0.150,0.3810)(0.175,0.4676)(0.200,0.4916)(0.300,0.5803)(0.400,0.6250)(0.500,0.6604)(0.600,0.6765)(0.700,0.7017)(0.800,0.7232)(0.900,0.7418)};
        		\addplot[color=blue, mark=square*]
        			coordinates {(0.100,0.3914)(0.125,0.5099)(0.150,0.6125)(0.175,0.7046)(0.200,0.7339)(0.300,0.8186)(0.400,0.8466)(0.500,0.8830)(0.600,0.8900)(0.700,0.9024)(0.800,0.9179)(0.900,0.9224)};
                    \addplot[color=blue, mark=square*, dashed]
        			coordinates {(0.025,0.7949)(0.050,0.6085)(0.075,0.5359)(0.100,0.3914)};
        		\legend{Self-Supervised (Top-1), Self-Supervised (Top-3), Supervised (Top-1), Supervised (Top-3)}
        	\end{axis}
        \end{tikzpicture}
        \caption{Self-supervised model performs better overall for all dataset splits in top-1 and top-3 accuracy, especially when the number of training samples approaches average of less 10 samples for a class with portion equal to 0.025. For training portion of less than 0.1, the top-3 accuracy is degraded. This section of the plot is marked with a dashed line.}
        \label{fig:self-supervised-vs-supervised-bent}
    \end{subfigure}
    \caption{Performance of self-supervised and supervised models on validation dataset based on what fraction of the whole dataset was used for training. The rest of the dataset was used for validation. Each data point is an average of 10 runs.}
    \label{fig:self-supervised-vs-supervised}
\end{figure*}
